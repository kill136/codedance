2025-01-18 15:17:32,582 [INFO] 设置 pad_token 为 eos_token: </s>
2025-01-18 15:17:34,482 [INFO] 成功加载预训练权重: ./out/pretrain.pth
2025-01-18 15:17:34,482 [INFO] LLM总参数量：26.878 百万
2025-01-18 15:17:34,751 [INFO] 处理完成代码文件数量: 5655
2025-01-18 15:17:36,284 [INFO] Epoch 0, Batch 0, Loss: 5.7831
2025-01-18 15:17:37,810 [INFO] Epoch 0, Batch 10, Loss: 4.1795
2025-01-18 15:17:39,339 [INFO] Epoch 0, Batch 20, Loss: 3.4555
2025-01-18 15:17:40,868 [INFO] Epoch 0, Batch 30, Loss: 2.6515
2025-01-18 15:17:42,398 [INFO] Epoch 0, Batch 40, Loss: 2.4243
2025-01-18 15:17:43,929 [INFO] Epoch 0, Batch 50, Loss: 2.1990
2025-01-18 15:17:45,459 [INFO] Epoch 0, Batch 60, Loss: 2.1026
2025-01-18 15:17:46,993 [INFO] Epoch 0, Batch 70, Loss: 2.0002
2025-01-18 15:17:48,528 [INFO] Epoch 0, Batch 80, Loss: 1.8419
2025-01-18 15:17:50,064 [INFO] Epoch 0, Batch 90, Loss: 1.7083
2025-01-18 15:17:51,602 [INFO] Epoch 0, Batch 100, Loss: 1.7280
2025-01-18 15:17:53,137 [INFO] Epoch 0, Batch 110, Loss: 1.6657
2025-01-18 15:17:54,674 [INFO] Epoch 0, Batch 120, Loss: 1.6270
2025-01-18 15:17:56,212 [INFO] Epoch 0, Batch 130, Loss: 1.5685
2025-01-18 15:17:57,744 [INFO] Epoch 0, Batch 140, Loss: 1.6027
2025-01-18 15:17:59,276 [INFO] Epoch 0, Batch 150, Loss: 1.5426
2025-01-18 15:18:00,809 [INFO] Epoch 0, Batch 160, Loss: 1.3878
2025-01-18 15:18:02,343 [INFO] Epoch 0, Batch 170, Loss: 1.5026
2025-01-18 15:18:03,278 [INFO] Epoch 0 Average Loss: 2.1836
2025-01-18 15:18:04,710 [INFO] Epoch 1, Batch 0, Loss: 1.3916
2025-01-18 15:18:06,246 [INFO] Epoch 1, Batch 10, Loss: 1.5390
2025-01-18 15:18:07,785 [INFO] Epoch 1, Batch 20, Loss: 1.1827
2025-01-18 15:18:09,321 [INFO] Epoch 1, Batch 30, Loss: 1.4178
2025-01-18 15:18:10,859 [INFO] Epoch 1, Batch 40, Loss: 1.3191
2025-01-18 15:18:12,397 [INFO] Epoch 1, Batch 50, Loss: 1.3375
2025-01-18 15:18:13,935 [INFO] Epoch 1, Batch 60, Loss: 1.1593
2025-01-18 15:18:15,475 [INFO] Epoch 1, Batch 70, Loss: 1.4290
2025-01-18 15:18:17,015 [INFO] Epoch 1, Batch 80, Loss: 1.4250
2025-01-18 15:18:18,556 [INFO] Epoch 1, Batch 90, Loss: 1.3701
2025-01-18 15:18:20,097 [INFO] Epoch 1, Batch 100, Loss: 1.3426
2025-01-18 15:18:21,639 [INFO] Epoch 1, Batch 110, Loss: 1.2447
2025-01-18 15:18:23,182 [INFO] Epoch 1, Batch 120, Loss: 1.3820
2025-01-18 15:18:24,724 [INFO] Epoch 1, Batch 130, Loss: 1.2200
2025-01-18 15:18:26,267 [INFO] Epoch 1, Batch 140, Loss: 1.2971
2025-01-18 15:18:27,811 [INFO] Epoch 1, Batch 150, Loss: 1.3611
2025-01-18 15:18:29,356 [INFO] Epoch 1, Batch 160, Loss: 1.0855
2025-01-18 15:18:30,899 [INFO] Epoch 1, Batch 170, Loss: 1.1467
2025-01-18 15:18:31,838 [INFO] Epoch 1 Average Loss: 1.3377
2025-01-18 15:18:33,222 [INFO] Epoch 2, Batch 0, Loss: 1.3361
2025-01-18 15:18:34,766 [INFO] Epoch 2, Batch 10, Loss: 1.2585
2025-01-18 15:18:36,311 [INFO] Epoch 2, Batch 20, Loss: 1.1683
2025-01-18 15:18:37,854 [INFO] Epoch 2, Batch 30, Loss: 1.0979
2025-01-18 15:18:39,398 [INFO] Epoch 2, Batch 40, Loss: 1.3308
2025-01-18 15:18:40,942 [INFO] Epoch 2, Batch 50, Loss: 1.2991
2025-01-18 15:18:42,486 [INFO] Epoch 2, Batch 60, Loss: 1.2242
2025-01-18 15:18:44,031 [INFO] Epoch 2, Batch 70, Loss: 1.0801
2025-01-18 15:18:45,575 [INFO] Epoch 2, Batch 80, Loss: 1.1304
2025-01-18 15:18:47,120 [INFO] Epoch 2, Batch 90, Loss: 1.2719
2025-01-18 15:18:48,666 [INFO] Epoch 2, Batch 100, Loss: 1.2341
2025-01-18 15:18:50,211 [INFO] Epoch 2, Batch 110, Loss: 1.2274
2025-01-18 15:18:51,757 [INFO] Epoch 2, Batch 120, Loss: 1.4822
2025-01-18 15:18:53,301 [INFO] Epoch 2, Batch 130, Loss: 1.3719
2025-01-18 15:18:54,847 [INFO] Epoch 2, Batch 140, Loss: 1.2667
2025-01-18 15:18:56,393 [INFO] Epoch 2, Batch 150, Loss: 1.2580
2025-01-18 15:18:57,937 [INFO] Epoch 2, Batch 160, Loss: 1.3162
2025-01-18 15:18:59,481 [INFO] Epoch 2, Batch 170, Loss: 1.0606
2025-01-18 15:19:00,440 [INFO] Epoch 2 Average Loss: 1.2258
